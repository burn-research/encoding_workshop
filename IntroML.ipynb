{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A gentle introduction to Machine Learning\n",
    "\n",
    "If you ask ChatGPT to describe ML you get something like this:\n",
    "\n",
    "*Machine learning is a subset of artificial intelligence (AI) that focuses on developing algorithms and models that enable computers to learn and make predictions or decisions without being explicitly programmed for a particular task.*\n",
    "\n",
    "Generally, ML contains two parts:\n",
    " - Selection of the model.\n",
    " - Tuning of the model's parameters.\n",
    "\n",
    "If you know something about Bayesian statistic, this should sound familiar.\n",
    "\n",
    "## Bayesian representation of Machine Learning\n",
    "\n",
    "Bayesian probability is a framework in which the uncertainty is interpreted as a the degree of knowledge on a subject (prior), and the data is used to update this knowledge (posterior).\n",
    "\n",
    "Let's take for example a simple regression problem:\n",
    "\n",
    "\\begin{equation}\n",
    "    y = f(\\mathbf{x}) + \\epsilon.\n",
    "\\end{equation}\n",
    "\n",
    "we want to find the function $f$ that relates our input $\\mathbf{x}$ to the measured output $y$. But we also know that there are some uncertainties in our observation, that we indicate with $\\epsilon$.\n",
    "\n",
    "If we assume that the function $f$ is a linear function, and there is no noise, we can represent the problem as:\n",
    "\n",
    "\\begin{equation}\n",
    "    y = \\mathbf{x} \\mathbf{w} + w_0\n",
    "\\end{equation}\n",
    "\n",
    "The prior is the choice of the linear model, and the data is used to select the weights $\\mathbf{w}$ of the model.\n",
    "\n",
    "The weights are chosen by minimising some error metric, like the $l_2$ norm:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\mathrm{arg min} || y - (\\mathbf{x}\\mathbf{w} + w_0)||^2_2\n",
    "\\end{equation}\n",
    "\n",
    "## Example\n",
    "Let's make a simple example, in which we try to predict the NO emissions given the equivalence ratio.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A quick example\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = pd.read_csv('emissions_furnace.csv')\n",
    "x = data[['phi']]\n",
    "y = data[['NO']]\n",
    "\n",
    "plt.scatter(x['phi'], y)\n",
    "plt.xlabel('phi')\n",
    "plt.ylabel('NO')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "x = np.array(x)\n",
    "y = np.array(y)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=12345)\n",
    "\n",
    "x0_train = (x_train - np.mean(x_train))/np.std(x_train)\n",
    "x0_test = (x_test - np.mean(x_train))/np.std(x_train)\n",
    "\n",
    "lr = LinearRegression().fit(x0_train, y_train)\n",
    "y_pred = lr.predict(x0_test)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "plt.scatter(y_test, y_pred)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], ls='--', c='r')\n",
    "plt.title(f'R2 = {r2:.2f}')\n",
    "plt.xlabel('Observed')\n",
    "plt.ylabel('Predicted')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What methods do we use to find the optimal models parameters?\n",
    "\n",
    "Depending on the type of the problem, the optimisation of the parameter can be done in different ways.\n",
    "If the problem is convex, the optimisation can be done using convex optimisation methods (CVXPY).\n",
    "\n",
    "This choice has some important benefits:\n",
    "- The solution is global\n",
    "- We can put (linear) constraints on the parameters\n",
    "\n",
    "### Example using CVXPY\n",
    "\n",
    "We solve the same linear regression problem using convex optimisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cvxpy as cp\n",
    "\n",
    "w = cp.Variable()\n",
    "w0 = cp.Variable()\n",
    "objective = cp.Minimize(cp.sum_squares(y_train - (x0_train * w + w0)))\n",
    "prob = cp.Problem(objective)\n",
    "\n",
    "result = prob.solve(verbose=True)\n",
    "\n",
    "y_pred = x0_test * w.value + w0.value\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "plt.scatter(y_test, y_pred)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], ls='--', c='r')\n",
    "plt.title(f'R2 = {r2:.2f}')\n",
    "plt.xlabel('Observed')\n",
    "plt.ylabel('Predicted')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cool thing about CVXPY is that we can easily modify the objective function, or we can introduce some constraints in the solution. For example, let's say that we know that the NO concentration cannot be greater than 250 ppm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = cp.Variable()\n",
    "w0 = cp.Variable()\n",
    "objective = cp.Minimize(cp.sum_squares(y_train - (x0_train * w + w0)))\n",
    "constraints = [x0_train * w + w0 <= 250]\n",
    "prob = cp.Problem(objective, constraints)\n",
    "\n",
    "result = prob.solve(verbose=False)\n",
    "\n",
    "y_pred = x0_test * w.value + w0.value\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "plt.scatter(y_test, y_pred)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], ls='--', c='r')\n",
    "plt.title(f'R2 = {r2:.2f}')\n",
    "plt.xlabel('Observed')\n",
    "plt.ylabel('Predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also modify the objective functions to include a penalty for the model's complexity (regularisation). For example, we can put a penalty on the $l_2$ norm of the weights. This is called Ridge regularisation, and it is helpful when the input features are correlated (co-linearity):\n",
    "\n",
    "\\begin{equation}\n",
    "    \\mathrm{arg min} || y - (\\mathbf{x}\\mathbf{w} + w_0)||^2_2 + \\alpha||\\mathbf{w}||^2_2\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = cp.Variable()\n",
    "w0 = cp.Variable()\n",
    "\n",
    "alpha = 0.1\n",
    "objective = cp.Minimize(cp.sum_squares(y_train - (x0_train * w + w0)) + alpha*cp.pnorm(w, p=2)**2)\n",
    "prob = cp.Problem(objective)\n",
    "\n",
    "result = prob.solve(verbose=False, solver='CLARABEL')\n",
    "\n",
    "y_pred = x0_test * w.value + w0.value\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "plt.scatter(y_test, y_pred)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], ls='--', c='r')\n",
    "plt.title(f'R2 = {r2:.2f}')\n",
    "plt.xlabel('Observed')\n",
    "plt.ylabel('Predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And if the problem is not convex?\n",
    "\n",
    "If your objective function is not convex, the other option is to use local methods such the gradient descent.Basically, to find the (local) minimum of the loss function, we follow the direction where the gradient is steepest.\n",
    "This methodology is the most common when training neural networks.\n",
    "\n",
    "## Example using Pytorch\n",
    "\n",
    "In this example we show how we can use a neural network to solve a OLS, regression problem, and how we can modify it to introduce a non-linear behaviour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define a simple linear neural network\n",
    "class LinearNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinearNN, self).__init__()\n",
    "        # Define the linear layer with two input nodes and one output node\n",
    "        self.linear = nn.Linear(1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply the linear activation\n",
    "        output = self.linear(x)\n",
    "        return output\n",
    "\n",
    "# Create an instance of the simple neural network\n",
    "model_lin = LinearNN()\n",
    "\n",
    "# Define a loss function (mean squared error)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = optim.SGD(model_lin.parameters(), lr=0.1)\n",
    "\n",
    "x0_train_torch = torch.from_numpy(x0_train).float()\n",
    "x0_test_torch = torch.from_numpy(x0_test).float()\n",
    "y_train_torch = torch.from_numpy(y_train).float()\n",
    "y_test_torch = torch.from_numpy(y_test).float()\n",
    "\n",
    "n_iter = 1000\n",
    "loss_array = np.zeros((n_iter,))\n",
    "val_loss_array = np.zeros((n_iter,))\n",
    "\n",
    "for i in range(n_iter):\n",
    "    optimizer.zero_grad()  # Zero the gradients to avoid accumulation\n",
    "    output = model_lin(x0_train_torch)\n",
    "    loss = criterion(output, y_train_torch)\n",
    "    loss_array[i] = loss.item()\n",
    "    \n",
    "    output_val = model_lin(x0_test_torch)\n",
    "    val_loss = criterion(output_val, y_test_torch)\n",
    "    val_loss_array[i] = val_loss.item()\n",
    "\n",
    "    # Perform backpropagation and update the weights\n",
    "    loss.backward()  # Compute gradients\n",
    "    optimizer.step()  # Update weights\n",
    "\n",
    "plt.plot(loss_array, label='loss')\n",
    "plt.plot(val_loss_array, label='val. loss')\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Check the updated output after training\n",
    "y_pred = model_lin(x0_test_torch).detach().numpy()\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "plt.scatter(y_test, y_pred)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], ls='--', c='r')\n",
    "plt.title(f'R2 = {r2:.2f}')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To introduce the non-linearity, we create a hidden layer with two nodes inside the neural network, using the Sigmoid activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class NonLinearNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NonLinearNN, self).__init__()\n",
    "        self.linear1 = nn.Linear(1, 2)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.linear2 = nn.Linear(2,1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        hidden = self.sigmoid(self.linear1(x))\n",
    "        output = self.linear2(hidden)\n",
    "        return output\n",
    "\n",
    "model_nl = NonLinearNN()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model_nl.parameters(), lr=0.001)\n",
    "\n",
    "n_iter = 5000\n",
    "loss_array = np.zeros((n_iter,))\n",
    "val_loss_array = np.zeros((n_iter,))\n",
    "\n",
    "for i in range(n_iter):\n",
    "    optimizer.zero_grad()  # Zero the gradients to avoid accumulation\n",
    "    output = model_nl(x0_train_torch)\n",
    "    loss = criterion(output, y_train_torch)\n",
    "    loss_array[i] = loss.item()\n",
    "\n",
    "    output_val = model_nl(x0_test_torch)\n",
    "    val_loss = criterion(output_val, y_test_torch)\n",
    "    val_loss_array[i] = val_loss.item()\n",
    "\n",
    "    loss.backward()  # Compute gradients\n",
    "    optimizer.step()  # Update weights\n",
    "\n",
    "plt.plot(loss_array)\n",
    "plt.plot(val_loss_array)\n",
    "plt.yscale('log')\n",
    "plt.show()\n",
    "\n",
    "y_pred = model_nl(x0_test_torch).detach().numpy()\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "plt.scatter(y_test, y_pred)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], ls='--', c='r')\n",
    "plt.title(f'R2 = {r2:.2f}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_new = np.linspace(x_train.min(), x_train.max(), 100)\n",
    "x0_new = (x_new - np.mean(x_train))/np.std(x_train)\n",
    "x0_new_torch = torch.from_numpy(x0_new.reshape(-1, 1)).float()\n",
    "\n",
    "y_new_ls = lr.predict(x0_new.reshape(-1, 1))\n",
    "y_new_nn = model_nl(x0_new_torch).detach().numpy()\n",
    "\n",
    "plt.plot(x0_new, y_new_ls)\n",
    "plt.plot(x0_new, y_new_nn)\n",
    "plt.scatter(x0_train, y_train)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A more Bayesian framework\n",
    "\n",
    "Gaussian Process Regression is a framework in which we assume that the function that we want to model is a sample from a multivariate Gaussian distribution.\n",
    "\n",
    "So we assume that the prior is a multivariate Gaussian distribution with zero mean, and the covariance matrix is computed using a kernel function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gpytorch\n",
    "from gpytorch.distributions import MultivariateNormal\n",
    "\n",
    "y0_train = (y_train - np.mean(y_train))/np.std(y_train)\n",
    "y0_train_torch = torch.from_numpy(y0_train).float()\n",
    "\n",
    "# The output of model is a gaussian distribution that we can sample\n",
    "\n",
    "mean = gpytorch.means.ConstantMean()\n",
    "kernel = gpytorch.kernels.RBFKernel()\n",
    "kernel.lengthscale = 1.\n",
    "mvdist = MultivariateNormal(mean(x0_new_torch), \n",
    "                            kernel(x0_new_torch))\n",
    "\n",
    "n_samples = 30\n",
    "samples = mvdist.sample(torch.Size([n_samples]))\n",
    "\n",
    "for i in range(n_samples):\n",
    "    plt.plot(x0_new_torch, samples[i, :], c='k', alpha=0.3)\n",
    "plt.scatter(x0_train, y0_train)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training is done to select the optimal hyperparameters, by maximizing the marginal log likelihood.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(GPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.RBFKernel()\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "# initialize likelihood and model\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "model_gp = GPModel(x0_train_torch.flatten(), y0_train_torch.flatten(), likelihood)\n",
    "\n",
    "# Find optimal model hyperparameters\n",
    "model_gp.train()\n",
    "likelihood.train()\n",
    "\n",
    "# Use the adam optimizer\n",
    "optimizer = torch.optim.Adam(model_gp.parameters(), lr=0.1)  # Includes GaussianLikelihood parameters\n",
    "\n",
    "# \"Loss\" for GPs - the marginal log likelihood\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model_gp)\n",
    "\n",
    "n_iter = 100\n",
    "loss_array = np.zeros((n_iter,))\n",
    "for i in range(n_iter):\n",
    "    optimizer.zero_grad()\n",
    "    output = model_gp(x0_train_torch.flatten())\n",
    "    loss = -mll(output, y0_train_torch.flatten())\n",
    "    loss_array[i] = loss.item()\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "plt.plot(loss_array)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we compute the predictive posterior by conditioning the joint distribution between the training and new data on the observed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_gp.eval()\n",
    "likelihood.eval()\n",
    "\n",
    "y_pred = np.std(y_test)*model_gp(x0_test_torch).mean.detach().numpy() + np.mean(y_train)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "plt.scatter(y_test, y_pred)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], ls='--', c='r')\n",
    "plt.title(f'R2 = {r2:.2f}')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "n_samples = 30\n",
    "samples = model_gp(x0_new_torch).sample(torch.Size([n_samples]))\n",
    "\n",
    "for i in range(n_samples):\n",
    "    plt.plot(x0_new_torch, samples[i, :], c='k', alpha=0.25)\n",
    "plt.scatter(x0_train, y0_train)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent space representation\n",
    "\n",
    "When we deal with high dimensional data coming from a physical process, we can expect that there exist a low-dimensional manifold in which we can embed the original data.\n",
    "\n",
    "We can have an intuition of the latent space (the space of the low-dimensional manifold) by considering some images of a flame.\n",
    "\n",
    "### Example: latent-dimensional representation of a flame\n",
    "\n",
    "Let's consider 1024 consecutive images of a flame, where each image has a 32x32 resolution.\n",
    "Each image can be considered a point in a 32 * 32 = 1024 dimensional space. However, only an infinitesimally small part of this space represents flames. Statistically, the vast majority of the space is noise.\n",
    "\n",
    "We can say that the space is sparse, and we can leverage this sparsity to compress the dimensionality while preserving the original information.\n",
    "\n",
    "To do this we can use the POD to find a transforming basis that is trained to represent the flames. The new basis U is a proper basis, so it can represent all the 1024-dimensional points. However, it is optimised to represent the flames used to find the new basis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = np.load('X_100_r32.npy')\n",
    "\n",
    "resolution = (32,32)\n",
    "dt = 1/2500\n",
    "\n",
    "fig, axs = plt.subplots(ncols=5, figsize=(15,3))\n",
    "for i in range(5):\n",
    "    axs[i].imshow(X[:,5*i].reshape(resolution))\n",
    "    axs[i].set_xticks([])\n",
    "    axs[i].set_yticks([])\n",
    "    axs[i].set_title(f't = {5*i*dt:.2e}s')\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U, Sigma, Vt = np.linalg.svd(X)\n",
    "A = np.transpose(np.diag(Sigma) @ Vt)\n",
    "\n",
    "fig, axs = plt.subplots(ncols=5, figsize=(15,3))\n",
    "for i in range(5):\n",
    "    axs[i].imshow(U[:,i].reshape(resolution))\n",
    "    axs[i].set_xticks([])\n",
    "    axs[i].set_yticks([])\n",
    "    axs[i].set_title(f'Mode = {i}')\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By combining the POD modes we can efficiently reconstruct the flame using few scalars. However, the POD modes represent a basis for the 1024-dimensional space, so in theory we can use them to represent whatever we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def plot_rec():\n",
    "    fig, axs = plt.subplots(8, 2, figsize=(6, 24))\n",
    "    n_modes = [2**i for i in range(3, 11)]\n",
    "    for i, n in enumerate(n_modes):\n",
    "        axs[i,0].imshow(np.matmul(U[:, :n],a_0[:n]).reshape(resolution))\n",
    "        axs[i,1].imshow(np.matmul(U[:, :n],a_image[:n]).reshape(resolution))\n",
    "        \n",
    "        axs[i,0].set_title(f'{n} modes')\n",
    "        \n",
    "        for j in range(2):\n",
    "            axs[i,j].axes.get_xaxis().set_visible(False)\n",
    "            axs[i,j].axes.get_yaxis().set_visible(False)\n",
    "\n",
    "image = Image.open(f'monalisa.png').convert('L')\n",
    "image = np.array(image.resize((32, 32)))/255.\n",
    "\n",
    "U, Sigma, Vt = np.linalg.svd(X)\n",
    "A = np.transpose(np.diag(Sigma) @ Vt)\n",
    "\n",
    "a_0 = A[0,:]\n",
    "a_image = U.T @ image.flatten()\n",
    "\n",
    "plot_rec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the animation\n",
    "\n",
    "from matplotlib.animation import FuncAnimation \n",
    "from IPython.display import HTML\n",
    "\n",
    "def animate(i):\n",
    "    im.set_array(np.matmul(U[:, :i*16],a_image[:i*16]).reshape(resolution))\n",
    "    \n",
    "    return im\n",
    "\n",
    "fps = 25\n",
    "\n",
    "fig = plt.figure(figsize=(6,3), dpi=400)\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "\n",
    "im = ax.imshow(np.matmul(U[:, :1],a_image[:1]).reshape(resolution))\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "anim = FuncAnimation(fig, animate, frames=64)\n",
    "HTML(anim.to_jshtml())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_ml2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
